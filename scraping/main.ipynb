{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b942b26",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path().resolve() / \"src\"))\n",
    "\n",
    "from savers.mongo import MongoSaver\n",
    "from scrapers.rbc import RBCGetter, RBCParser\n",
    "from scrapers.habr import HabrGetter\n",
    "from config.config import Config\n",
    "from log import log\n",
    "\n",
    "from common import ParsedScrap, Scrap\n",
    "from config.config import SiteConfig\n",
    "from savers.interfaces import ISaver\n",
    "from scrapers.interfaces import IGetter, IParser\n",
    "\n",
    "import logging\n",
    "\n",
    "log.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08a0e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(Path().resolve() / \"..\" / \"scraper.yml\")\n",
    "log.info(f\"got config = {config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e7a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_first_n(set: set, n: int):\n",
    "    i = 0\n",
    "    for s in set:\n",
    "        if i >= n:\n",
    "            break\n",
    "        log.info(f\"\\t{s}\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97974f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENTITIES_LOG_COUNT: int = 0\n",
    "MAPPING: dict[str, tuple[type[IGetter], type[IParser]]] = {\n",
    "    \"rbc\": (RBCGetter, RBCParser),\n",
    "    \"habr\": (HabrGetter, RBCParser),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186f2b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = MongoSaver(config.mongo_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d8ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(getter: IGetter, cfg: SiteConfig) -> set[Scrap]:\n",
    "    log.info(f\"going to parse at most {cfg.doc_limit} documents\")\n",
    "    sources = getter.fetch_sources()\n",
    "    log.info(f\"first {ENTITIES_LOG_COUNT} sources = \")\n",
    "    log_first_n(sources, ENTITIES_LOG_COUNT)\n",
    "    log.info(\n",
    "        f\"fetched {len(sources)} articles, fetching all of them will take about {len(sources) / cfg.crawl_delay / 60 / 60:.3f} hours\"\n",
    "    )\n",
    "\n",
    "    text = getter.fetch_scrap(sources)\n",
    "    del sources\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse(parser: IParser, scrap: set[Scrap], cfg: SiteConfig) -> set[ParsedScrap]:\n",
    "    return parser.parse_scrap(scrap)\n",
    "\n",
    "\n",
    "def save(saver: ISaver, parsed_scrap: set[ParsedScrap], cfg: SiteConfig) -> None:\n",
    "    saver.save_parsed_scrap(parsed_scrap)\n",
    "\n",
    "\n",
    "for site, cfg in config.sites.items():\n",
    "    try:\n",
    "        log.info(f\"started scraping {site}\")\n",
    "        (Getter, Parser) = MAPPING[site]\n",
    "        getter = Getter(cfg)  # type: ignore\n",
    "        parser = Parser()\n",
    "\n",
    "        scrap = get(getter, cfg)\n",
    "        parser.info_scrap(scrap)\n",
    "\n",
    "        parsed_scrap = parse(parser, scrap, cfg)\n",
    "        del scrap\n",
    "        parser.info_parsed_scrap(parsed_scrap)\n",
    "        log.info(f\"first {ENTITIES_LOG_COUNT} parsed scraps = \")\n",
    "        log_first_n(set(map(lambda x: x.value, parsed_scrap)), ENTITIES_LOG_COUNT)\n",
    "\n",
    "        save(saver, parsed_scrap, cfg)\n",
    "\n",
    "        del parsed_scrap\n",
    "    except Exception as e:\n",
    "        log.error(f\"scraping {site} with cfg = {cfg} failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89718b6f",
   "metadata": {},
   "source": [
    "# Закон Ципфа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40037170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path().resolve() / \"src\"))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "from Stemmer import Stemmer\n",
    "from pymongo import MongoClient\n",
    "\n",
    "from log import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a617ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "RU_STEMMER = Stemmer(\"russian\")\n",
    "EN_STEMMER = Stemmer(\"english\")\n",
    "\n",
    "\n",
    "def load_texts_from_mongodb(\n",
    "    uri=\"mongodb://root:example@localhost:27017\", limit=5000\n",
    ") -> list[str]:\n",
    "    client = MongoClient(uri)\n",
    "\n",
    "    db = client[\"scraper\"]\n",
    "\n",
    "    collection = db[\"scraps\"]\n",
    "\n",
    "    texts = []\n",
    "    total_docs = collection.count_documents({})\n",
    "\n",
    "    log.info(f\"Найдено документов: {total_docs}\")\n",
    "\n",
    "    batch_size = 10000\n",
    "    processed = 0\n",
    "\n",
    "    for doc in collection.find({}, {\"value\": 1}).batch_size(batch_size):\n",
    "        if \"value\" in doc and doc[\"value\"]:\n",
    "            texts.append(doc[\"value\"])\n",
    "\n",
    "        processed += 1\n",
    "        if processed % 1000 == 0:\n",
    "            log.info(f\"Обработано {processed}/{total_docs} документов\")\n",
    "        if processed >= limit:\n",
    "            break\n",
    "\n",
    "    client.close()\n",
    "    log.info(f\"Загружено {len(texts)} текстов\")\n",
    "\n",
    "    return texts\n",
    "\n",
    "\n",
    "def is_cyrillic(char: str) -> bool:\n",
    "    if len(char.encode(\"utf-8\")) >= 2:\n",
    "        bytes_val = char.encode(\"utf-8\")\n",
    "        if len(bytes_val) >= 2:\n",
    "            return \"\\u0400\" <= char <= \"\\u04ff\"\n",
    "    return False\n",
    "\n",
    "\n",
    "def normalize_text_utf8(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    result = []\n",
    "    i = 0\n",
    "    text_len = len(text)\n",
    "\n",
    "    while i < text_len:\n",
    "        char = text[i]\n",
    "        if ord(char) < 128:\n",
    "            if char.isalnum():\n",
    "                result.append(char.lower())\n",
    "            else:\n",
    "                result.append(\" \")\n",
    "            i += 1\n",
    "        else:\n",
    "            if is_cyrillic(char):\n",
    "                lower_char = char.lower()\n",
    "                result.append(lower_char)\n",
    "            else:\n",
    "                result.append(\" \")\n",
    "            i += 1\n",
    "\n",
    "    return \"\".join(result)\n",
    "\n",
    "\n",
    "def tokenize_and_stem(text: str) -> List[str]:\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    normalized = normalize_text_utf8(text)\n",
    "\n",
    "    tokens = []\n",
    "    current_token = []\n",
    "\n",
    "    for char in normalized:\n",
    "        if char != \" \":\n",
    "            current_token.append(char)\n",
    "        else:\n",
    "            if current_token:\n",
    "                token = \"\".join(current_token)\n",
    "                if token:\n",
    "                    tokens.append(token)\n",
    "                current_token = []\n",
    "\n",
    "    if current_token:\n",
    "        token = \"\".join(current_token)\n",
    "        if token:\n",
    "            tokens.append(token)\n",
    "\n",
    "    stemmed_tokens = []\n",
    "    for token in tokens:\n",
    "        has_cyrillic = any(is_cyrillic(c) for c in token)\n",
    "\n",
    "        if has_cyrillic:\n",
    "            stemmed = RU_STEMMER.stemWord(token)\n",
    "        else:\n",
    "            stemmed = EN_STEMMER.stemWord(token)\n",
    "\n",
    "        if len(stemmed) > 2:\n",
    "            stemmed_tokens.append(stemmed)\n",
    "\n",
    "    return stemmed_tokens\n",
    "\n",
    "\n",
    "def build_zipf_law(tokens: List[str]):\n",
    "    word_counts = Counter(tokens)\n",
    "\n",
    "    sorted_counts = word_counts.most_common()\n",
    "\n",
    "    ranks = list(range(1, len(sorted_counts) + 1))\n",
    "    frequencies = [count for _, count in sorted_counts]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    plt.loglog(ranks, frequencies, \"ro-\", linewidth=2, alpha=0.7, markersize=4)\n",
    "    plt.xlabel(\"Ранг слова (log)\", fontsize=12)\n",
    "    plt.ylabel(\"Частота (log)\", fontsize=12)\n",
    "    plt.title(\"Закон Ципфа (логарифмический масштаб)\", fontsize=14)\n",
    "    plt.grid(True, alpha=0.3, which=\"both\")\n",
    "\n",
    "    if len(ranks) > 1:\n",
    "        zipf_freq = [frequencies[0] / r for r in ranks]\n",
    "        plt.loglog(\n",
    "            ranks,\n",
    "            zipf_freq,\n",
    "            \"g--\",\n",
    "            linewidth=1,\n",
    "            alpha=0.5,\n",
    "            label=\"Теоретическая кривая Ципфа\",\n",
    "        )\n",
    "        plt.legend()\n",
    "\n",
    "    plt.suptitle(\n",
    "        f\"Закон Ципфа\\nВсего уникальных слов: {len(sorted_counts)} | Всего токенов: {len(tokens)}\",\n",
    "        fontsize=16,\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Всего токенов: {len(tokens)}\")\n",
    "    print(f\"Уникальных токенов: {len(sorted_counts)}\")\n",
    "    print(f\"Средняя длина токена: {sum(map(len, tokens)) / len(tokens)}\")\n",
    "\n",
    "    print(\"\\nТоп-10 самых частых слов:\")\n",
    "    for i, (word, count) in enumerate(sorted_counts[:10], 1):\n",
    "        percentage = (count / len(tokens)) * 100\n",
    "        print(f\"{i:2d}. {word:20s} - {count:6d} ({percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4081ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = load_texts_from_mongodb(limit=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefa655",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize_and_stem(\" \".join(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0ba00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_zipf_law(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
